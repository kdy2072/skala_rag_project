import os
from glob import glob
from typing import List
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

class VectorDBBuilder:
    def __init__(self, model_name: str = "nlpai-lab/KURE-v1"):
        """
        :param model_name: HuggingFace ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.embedding_model = HuggingFaceEmbeddings(model_name=model_name)

    def build_from_pdfs(self, pdf_files: List[str], save_path: str = None) -> FAISS:
        """
        PDF íŒŒì¼ë“¤ë¡œë¶€í„° ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ FAISS ë²¡í„°DBë¥¼ ìƒì„±
        :param pdf_files: PDF íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        :param save_path: ì €ì¥í•  ê²½ë¡œ (ì˜ˆ: 'faiss_db/unicorns')
        :return: FAISS ê°ì²´
        """
        docs = []
        total_pages = 0
        total_chunks = 0

        # âœ¨ Semantic Chunking ì ìš©
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            separators=["\n\n", "\n", " ", ""],  # ë¬¸ë‹¨/ë¬¸ì¥ ë‹¨ìœ„ ë³´ì¡´
            length_function=len,
            is_separator_regex=False
        )

        for pdf in pdf_files:
            company_name = os.path.basename(pdf).split('.')[0]  # íŒŒì¼ëª… ê¸°ë°˜ íšŒì‚¬ëª… ì¶”ì¶œ
            loader = PyPDFLoader(pdf)
            pages = loader.load()
            total_pages += len(pages)

            # Semantic Chunking ì‹¤í–‰
            splits = splitter.split_documents(pages)
            total_chunks += len(splits)

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            for split in splits:
                split.metadata["company"] = company_name

            print(f"ğŸ“„ {os.path.basename(pdf)} ({company_name}) â†’ {len(pages)} pages â†’ {len(splits)} chunks")

            docs.extend(splits)

        # âœ¨ Flat Index (FAISS ê¸°ë³¸: IndexFlatL2 / IP)
        vectordb = FAISS.from_documents(docs, self.embedding_model)

        if save_path:
            os.makedirs(save_path, exist_ok=True)
            vectordb.save_local(save_path)

        print("=====================================")
        print(f"âœ… ì´ PDF ê°œìˆ˜: {len(pdf_files)}")
        print(f"âœ… ì´ í˜ì´ì§€ ìˆ˜: {total_pages}")
        print(f"âœ… ì´ ì²­í¬ ìˆ˜: {total_chunks}")
        print(f"âœ… ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì™„ë£Œ: 'company'")
        print(f"âœ… ë²¡í„°DB ì €ì¥ ê²½ë¡œ: {save_path}")
        print("=====================================")

        return vectordb

    def load_vectorstore(self, save_path: str) -> FAISS:
        """
        ì €ì¥ëœ FAISS ë²¡í„°DB ë¡œë“œ
        """
        return FAISS.load_local(
            save_path,
            self.embedding_model,
            allow_dangerous_deserialization=True
        )


if __name__ == "__main__":
    # ğŸ“‚ data í´ë” ë°‘ ëª¨ë“  PDF ìë™ íƒì§€
    data_dir = "data"
    pdf_files = glob(os.path.join(data_dir, "*.pdf"))

    if not pdf_files:
        print("âš ï¸ data í´ë”ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        exit()

    # ğŸ“‚ ì €ì¥ ê²½ë¡œ
    save_path = "faiss_db/unicorns_sementic"

    # ğŸš€ VectorDB ìƒì„±
    builder = VectorDBBuilder(model_name="nlpai-lab/KURE-v1")
    vectordb = builder.build_from_pdfs(pdf_files, save_path)
