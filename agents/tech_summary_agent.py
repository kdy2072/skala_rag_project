import json
import logging
from typing import Dict, List, Optional, Any
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.tools.tavily_search import TavilySearchResults
import os

# ê°„ë‹¨í•œ ìƒíƒœ ì •ì˜ (LangGraph ëŒ€ì‹ )
class TechAnalysisState:
    def __init__(self, **kwargs):
        for key, value in kwargs.items():
            setattr(self, key, value)
    
    def get(self, key, default=None):
        return getattr(self, key, default)
    
    def update(self, data):
        for key, value in data.items():
            setattr(self, key, value)

class TechExistenceValidatorTool:
    """ê¸°ìˆ  ì‹¤ì¡´ì„± ê²€ì¦ ë„êµ¬ - ì •ì œëœ ë°ì´í„° + ì›¹ê²€ìƒ‰"""
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.web_search = TavilySearchResults(max_results=3)
        
    def run(self, state: TechAnalysisState) -> TechAnalysisState:
        """ì •ì œëœ ë°ì´í„° + ì›¹ê²€ìƒ‰ìœ¼ë¡œ ê¸°ìˆ  ì‹¤ì¡´ì„± ê²€ì¦"""
        print(f"ğŸ” ê¸°ìˆ  ì‹¤ì¡´ì„± ê²€ì¦: {state.get('company_name', '')} - {state.get('core_tech', '')}")
        
        try:
            company_name = state.get("company_name", "")
            core_tech = state.get("core_tech", "")
            patents = state.get("patents", "")
            investments = state.get("investments", "")
            pros = state.get("pros", "")
            owner = state.get("owner", "")
            
            # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì¶”ê°€ ê²€ì¦
            web_results = []
            try:
                web_query = f'"{company_name}" "{core_tech}" technology patent'
                web_results = self.web_search.run(web_query)
                print("âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            # ì •êµí•œ ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self._calculate_confidence_score(
                web_results, company_name, core_tech, patents, investments, pros, owner
            )
            
            tech_exists = confidence >= 70
            
            state.tech_exists = tech_exists
            state.tech_cache = {
                "confidence": confidence,
                "web_evidence": str(web_results)[:300],
                "confidence_breakdown": self._get_confidence_breakdown(confidence)
            }
            
            print(f"âœ… ì‹¤ì¡´ì„± ê²€ì¦ ì™„ë£Œ: {tech_exists} (í™•ì‹ ë„: {confidence:.1f}%)")
            
        except Exception as e:
            print(f"âŒ ê²€ì¦ ì˜¤ë¥˜: {e}")
            state.tech_exists = True  # ì •ì œëœ ë°ì´í„°ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì‹ ë¢°
            state.tech_cache = {"error": str(e), "confidence": 80}
        
        return state
    
    def _calculate_confidence_score(self, web_results, company_name, core_tech, patents, investments, pros, owner):
        """ì •êµí•œ ì‹ ë¢°ë„ ê³„ì‚°"""
        # 1. ì›¹ê²€ìƒ‰ í’ˆì§ˆ (40%)
        web_quality_score = self._evaluate_web_search_quality(web_results, company_name, core_tech)
        
        # 2. ë°ì´í„° ì™„ì„±ë„ (35%)
        data_completeness_score = self._evaluate_data_completeness(company_name, core_tech, pros, owner)
        
        # 3. íŠ¹í—ˆ/íˆ¬ì ì •ë³´ (25%)
        patent_investment_score = self._evaluate_patent_investment_info(patents, investments)
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_confidence = (
            web_quality_score * 0.4 +
            data_completeness_score * 0.35 +
            patent_investment_score * 0.25
        )
        
        return min(100, max(0, total_confidence))
    
    def _evaluate_web_search_quality(self, web_results, company_name, core_tech):
        """ì›¹ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ (0-100ì )"""
        if not web_results:
            return 20
        
        score = 0
        
        # ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ (30ì )
        result_count = len(web_results)
        if result_count >= 3:
            score += 30
        elif result_count >= 2:
            score += 20
        elif result_count >= 1:
            score += 10
        
        # ê²°ê³¼ ë‚´ìš©ì˜ ê´€ë ¨ì„± (40ì )
        relevant_results = 0
        for result in web_results:
            result_text = str(result).lower()
            if company_name.lower() in result_text and core_tech.lower() in result_text:
                relevant_results += 1
        
        if relevant_results >= 2:
            score += 40
        elif relevant_results >= 1:
            score += 25
        
        # ì‹ ë¢°í•  ë§Œí•œ ì†ŒìŠ¤ ì—¬ë¶€ (30ì )
        trusted_sources = ['news', 'patent', 'research', 'official', 'company', 'tech']
        trusted_count = 0
        for result in web_results:
            result_text = str(result).lower()
            if any(source in result_text for source in trusted_sources):
                trusted_count += 1
        
        if trusted_count >= 2:
            score += 30
        elif trusted_count >= 1:
            score += 15
        
        return min(100, score)
    
    def _evaluate_data_completeness(self, company_name, core_tech, pros, owner):
        """ë°ì´í„° ì™„ì„±ë„ í‰ê°€ (0-100ì )"""
        score = 0
        
        # í•„ìˆ˜ í•„ë“œ ì¡´ì¬ ì—¬ë¶€ (40ì )
        required_fields = [company_name, core_tech]
        filled_required = sum(1 for field in required_fields if field and len(field.strip()) > 0)
        score += (filled_required / len(required_fields)) * 40
        
        # ê° í•„ë“œì˜ ì •ë³´ëŸ‰ (35ì )
        optional_fields = [pros, owner]
        total_length = sum(len(field) for field in [company_name, core_tech, pros, owner] if field)
        if total_length >= 200:
            score += 35
        elif total_length >= 100:
            score += 25
        elif total_length >= 50:
            score += 15
        
        # êµ¬ì²´ì„± ìˆ˜ì¤€ (25ì )
        specificity_keywords = ['AI', 'ê¸°ìˆ ', 'ê°œë°œ', 'íŠ¹í—ˆ', 'ì—°êµ¬', 'ë°•ì‚¬', 'ëŒ€í•™êµ', 'ì „ë¬¸', 'í˜ì‹ ']
        specificity_count = 0
        all_text = f"{company_name} {core_tech} {pros} {owner}".lower()
        
        for keyword in specificity_keywords:
            if keyword.lower() in all_text:
                specificity_count += 1
        
        if specificity_count >= 5:
            score += 25
        elif specificity_count >= 3:
            score += 15
        elif specificity_count >= 1:
            score += 8
        
        return min(100, score)
    
    def _evaluate_patent_investment_info(self, patents, investments):
        """íŠ¹í—ˆ/íˆ¬ì ì •ë³´ í‰ê°€ (0-100ì )"""
        score = 0
        
        # íŠ¹í—ˆ ê±´ìˆ˜ ì–¸ê¸‰ (40ì )
        if patents:
            patent_numbers = [int(s) for s in patents.split() if s.isdigit()]
            if patent_numbers:
                max_patent = max(patent_numbers)
                if max_patent >= 15:
                    score += 40
                elif max_patent >= 10:
                    score += 30
                elif max_patent >= 5:
                    score += 20
                elif max_patent >= 1:
                    score += 10
            elif 'íŠ¹í—ˆ' in patents:
                score += 15
        
        # íˆ¬ì ê¸ˆì•¡/ë¼ìš´ë“œ ì •ë³´ (35ì )
        if investments:
            investment_keywords = ['ì–µ', 'ë§Œì›', 'Series', 'Pre', 'Bridge', 'íˆ¬ì', 'ìœ ì¹˜']
            investment_mentions = sum(1 for keyword in investment_keywords if keyword in investments)
            
            if investment_mentions >= 3:
                score += 35
            elif investment_mentions >= 2:
                score += 25
            elif investment_mentions >= 1:
                score += 15
        
        # êµ¬ì²´ì  ìˆ˜ì¹˜ í¬í•¨ ì—¬ë¶€ (25ì )
        all_info = f"{patents} {investments}"
        numbers = [int(s) for s in all_info.split() if s.isdigit()]
        
        if len(numbers) >= 3:
            score += 25
        elif len(numbers) >= 2:
            score += 18
        elif len(numbers) >= 1:
            score += 10
        
        return min(100, score)
    
    def _get_confidence_breakdown(self, total_confidence):
        """ì‹ ë¢°ë„ ì„¸ë¶€ ë¶„ì„ ì •ë³´"""
        if total_confidence >= 90:
            return "ë§¤ìš° ë†’ì€ ì‹ ë¢°ë„ - ìƒì„¸ ë¶„ì„ ê¶Œì¥"
        elif total_confidence >= 70:
            return "ë†’ì€ ì‹ ë¢°ë„ - ìƒì„¸ ë¶„ì„ ì§„í–‰"
        elif total_confidence >= 50:
            return "ì¤‘ê°„ ì‹ ë¢°ë„ - ê¸°ë³¸ ë¶„ì„ìœ¼ë¡œ ì œí•œ"
        else:
            return "ë‚®ì€ ì‹ ë¢°ë„ - ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ í•„ìš”"
    
    def _extract_confidence(self, text: str) -> int:
        """LLM ì‘ë‹µì—ì„œ ì‹ ë¢°ë„ ì¶”ì¶œ (ë ˆê±°ì‹œ ë©”ì„œë“œ)"""
        try:
            lines = text.split('\n')
            for line in lines:
                if 'CONFIDENCE:' in line:
                    return int(''.join(filter(str.isdigit, line)))
        except:
            pass
        return 80

class CoreTechAnalyzerTool:
    """í•µì‹¬ ê¸°ìˆ  ë¶„ì„ ë„êµ¬ - ì›¹ê²€ìƒ‰ ë³´ê°•"""
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.web_search = TavilySearchResults(max_results=3)
        
    def run(self, state: TechAnalysisState) -> TechAnalysisState:
        print("ğŸ”¬ í•µì‹¬ ê¸°ìˆ  ë¶„ì„ ì‹œì‘")
        
        try:
            company_name = state.get("company_name", "")
            core_tech = state.get("core_tech", "")
            pros = state.get("pros", "")
            
            # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ê¸°ìˆ  ì •ë³´ ë³´ê°•
            web_tech_info = ""
            try:
                tech_query = f'"{company_name}" "{core_tech}" how it works mechanism'
                web_results = self.web_search.run(tech_query)
                web_tech_info = "\n".join([str(result)[:200] for result in web_results])
                print("âœ… ê¸°ìˆ  ì •ë³´ ì›¹ ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê¸°ìˆ  ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            analysis_prompt = PromptTemplate(
                template="""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¸°ìˆ ì„ ì‹¬ì¸µ ë¶„ì„í•˜ì„¸ìš”:

íšŒì‚¬: {company}
í•µì‹¬ ê¸°ìˆ : {tech}
ê°•ì : {pros}

ì›¹ ê²€ìƒ‰ ì •ë³´:
{web_info}

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
1. ê¸°ìˆ ì˜ ì‘ìš© ì›ë¦¬ì™€ ë©”ì»¤ë‹ˆì¦˜ (300ì ì´ë‚´)
2. ì£¼ìš” ì ìš© ë¶„ì•¼ì™€ í™œìš© ê°€ëŠ¥ì„± (200ì ì´ë‚´)
3. ê¸°ìˆ ì  ì¥ì ê³¼ í˜ì‹ ì„± (250ì ì´ë‚´)

ì‘ë‹µ í˜•ì‹:
MECHANISM: ì‘ìš© ì›ë¦¬
APPLICATIONS: ì ìš© ë¶„ì•¼
ADVANTAGES: ê¸°ìˆ ì  ì¥ì """,
                input_variables=["company", "tech", "pros", "web_info"]
            )
            
            chain = analysis_prompt | self.llm | StrOutputParser()
            analysis = chain.invoke({
                "company": company_name,
                "tech": core_tech,
                "pros": pros,
                "web_info": web_tech_info[:800]
            })
            
            # ê¸°ì¡´ ìºì‹œì— ì¶”ê°€
            if not hasattr(state, 'tech_cache'):
                state.tech_cache = {}
            
            state.tech_cache.update({
                "mechanism": self._extract_section(analysis, "MECHANISM"),
                "applications": self._extract_section(analysis, "APPLICATIONS"),
                "advantages": self._extract_section(analysis, "ADVANTAGES"),
                "web_research": web_tech_info[:200]
            })
            
            print("âœ… í•µì‹¬ ê¸°ìˆ  ë¶„ì„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ê¸°ìˆ  ë¶„ì„ ì˜¤ë¥˜: {e}")
            if not hasattr(state, 'tech_cache'):
                state.tech_cache = {}
            state.tech_cache["analysis_error"] = str(e)
        
        return state
    
    def _extract_section(self, text: str, section: str) -> str:
        lines = text.split('\n')
        for line in lines:
            if section in line and ':' in line:
                return line.split(':', 1)[1].strip()
        return "ì •ë³´ ë¶€ì¡±"

class IP_ResearchValidatorTool:
    """ì§€ì ì¬ì‚°ê¶Œ ê²€ì¦ ë„êµ¬ - ì›¹ê²€ìƒ‰ êµì°¨ê²€ì¦"""
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.web_search = TavilySearchResults(max_results=3)
        
    def run(self, state: TechAnalysisState) -> TechAnalysisState:
        print("ğŸ“‹ IP ê²€ì¦ ì‹œì‘")
        
        try:
            company_name = state.get("company_name", "")
            patents_info = state.get("patents", "")
            
            # ì›¹ ê²€ìƒ‰ìœ¼ë¡œ íŠ¹í—ˆ ì •ë³´ êµì°¨ ê²€ì¦
            web_patent_info = ""
            try:
                patent_query = f'"{company_name}" patents intellectual property'
                web_results = self.web_search.run(patent_query)
                web_patent_info = "\n".join([str(result)[:200] for result in web_results])
                print("âœ… íŠ¹í—ˆ ì •ë³´ ì›¹ ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ íŠ¹í—ˆ ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            patent_prompt = PromptTemplate(
                template="""ë‹¤ìŒ íŠ¹í—ˆ ì •ë³´ë¥¼ ì¢…í•© ë¶„ì„í•˜ì„¸ìš”:

íšŒì‚¬: {company}
ê¸°ì¡´ íŠ¹í—ˆ ì •ë³´: {patents}

ì›¹ ê²€ìƒ‰ íŠ¹í—ˆ ì •ë³´:
{web_patents}

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
1. íŠ¹í—ˆ ê±´ìˆ˜ì™€ ë“±ë¡ í˜„í™©
2. ì£¼ìš” íŠ¹í—ˆ ê¸°ìˆ  ë¶„ì•¼
3. íŠ¹í—ˆì˜ ê¸°ìˆ ì  ê°€ì¹˜ì™€ ê²½ìŸë ¥

ì‘ë‹µ í˜•ì‹ (JSON):
{{
    "patent_count": ìˆ«ì,
    "main_fields": ["ë¶„ì•¼1", "ë¶„ì•¼2"],
    "registration_status": "ë“±ë¡/ì¶œì› í˜„í™©",
    "technical_value": "ê¸°ìˆ ì  ê°€ì¹˜ í‰ê°€",
    "competitive_strength": "íŠ¹í—ˆ ê²½ìŸë ¥"
}}""",
                input_variables=["company", "patents", "web_patents"]
            )
            
            chain = patent_prompt | self.llm | StrOutputParser()
            patent_analysis = chain.invoke({
                "company": company_name,
                "patents": patents_info,
                "web_patents": web_patent_info[:600]
            })
            
            try:
                # JSON íŒŒì‹± ì‹œë„
                patent_data = json.loads(patent_analysis.replace('```json', '').replace('```', '').strip())
            except:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
                patent_count = len([x for x in patents_info.split() if x.isdigit()])
                patent_data = {
                    "patent_count": patent_count if patent_count > 0 else 5,
                    "main_fields": ["AI/ML", "í—¬ìŠ¤ì¼€ì–´"],
                    "registration_status": "ë“±ë¡ ë° ì¶œì› ì§„í–‰",
                    "technical_value": "ì¤‘ê°„ ìˆ˜ì¤€ì˜ ê¸°ìˆ ì  ê°€ì¹˜",
                    "competitive_strength": "íŠ¹í—ˆ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì¶• ì¤‘"
                }
            
            state.ip_cache = {
                "patent_analysis": patent_data,
                "web_verification": web_patent_info[:300],
                "significance": "high" if patent_data.get("patent_count", 0) > 10 else "medium"
            }
            
            print("âœ… IP ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ IP ê²€ì¦ ì˜¤ë¥˜: {e}")
            state.ip_cache = {"error": str(e), "significance": "unknown"}
        
        return state

class CompetitiveLandscapeAgent:
    """ê²½ìŸ í™˜ê²½ ë¶„ì„ ì„œë¸Œì—ì´ì „íŠ¸ - ì›¹ê²€ìƒ‰ ê¸°ë°˜"""
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        self.web_search = TavilySearchResults(max_results=5)
        
    def run(self, state: TechAnalysisState) -> TechAnalysisState:
        print("ğŸ† ê²½ìŸê¸°ìˆ  ë¶„ì„ ì‹œì‘")
        
        try:
            company_name = state.get("company_name", "")
            core_tech = state.get("core_tech", "")
            
            # ê²½ìŸì‚¬ ê²€ìƒ‰
            competitors_info = ""
            try:
                comp_query = f'"{core_tech}" competitors market leaders companies'
                web_results = self.web_search.run(comp_query)
                competitors_info = "\n".join([str(result)[:200] for result in web_results])
                print("âœ… ê²½ìŸê¸°ìˆ  ì •ë³´ ê²€ìƒ‰ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ê²½ìŸê¸°ìˆ  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            competition_prompt = PromptTemplate(
                template="""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²½ìŸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”:

ë¶„ì„ ëŒ€ìƒ: {company}
í•µì‹¬ ê¸°ìˆ : {tech}

ê²½ìŸì‚¬ ì •ë³´:
{competitors}

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
1. ì£¼ìš” ê²½ìŸê¸°ìˆ  ì‹ë³„ (3-5ê°œ)
2. ê¸°ìˆ ì  ì°¨ë³„ì ê³¼ ê²½ìŸ ìš°ìœ„ (300ì ì´ë‚´)
3. ì‹œì¥ì—ì„œì˜ ê¸°ìˆ ì  í¬ì§€ì…”ë‹ (200ì ì´ë‚´)
4. ê²½ìŸ ì—´ì„¸ ìš”ì†Œ (150ì ì´ë‚´)

ì‘ë‹µ í˜•ì‹:
COMPETITORS: ì£¼ìš” ê´€ë ¨ ê²½ìŸê¸°ìˆ  ëª©ë¡
DIFFERENTIATION: ê¸°ìˆ ì  ì°¨ë³„ì 
ADVANTAGES: ê¸°ìˆ ì  ê²½ìŸ ìš°ìœ„
POSITIONING: ê¸°ìˆ ì  í¬ì§€ì…”ë‹
WEAKNESSES: ê¸°ìˆ ì  ê²½ìŸ ì—´ì„¸""",
                input_variables=["company", "tech", "competitors"]
            )
            
            chain = competition_prompt | self.llm | StrOutputParser()
            analysis = chain.invoke({
                "company": company_name,
                "tech": core_tech,
                "competitors": competitors_info[:800]
            })
            
            state.competition_cache = {
                "competitors": self._extract_competitors(analysis),
                "analysis": analysis,
                "differentiation": self._extract_section(analysis, "DIFFERENTIATION"),
                "web_research": competitors_info[:300]
            }
            
            print("âœ… ê²½ìŸ ë¶„ì„ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ê²½ìŸ ë¶„ì„ ì˜¤ë¥˜: {e}")
            state.competition_cache = {"error": str(e), "differentiation": "ë¶„ì„ ë¶ˆê°€"}
        
        return state
    
    def _extract_competitors(self, text: str) -> List[str]:
        competitors_line = self._extract_section(text, "COMPETITORS")
        if competitors_line and competitors_line != "ì •ë³´ ì—†ìŒ":
            return [comp.strip() for comp in competitors_line.split(',')[:5]]
        return ["ê²½ìŸì‚¬ A", "ê²½ìŸì‚¬ B", "ê²½ìŸì‚¬ C"]
    
    def _extract_section(self, text: str, section: str) -> str:
        lines = text.split('\n')
        for line in lines:
            if section in line and ':' in line:
                return line.split(':', 1)[1].strip()
        return "ì •ë³´ ì—†ìŒ"

class TechSummaryAgent:
    """ê¸°ìˆ  ìš”ì•½ ë©”ì¸ ì—ì´ì „íŠ¸ - Agentic ì›Œí¬í”Œë¡œìš°"""
    def __init__(self):
        self.tech_validator = TechExistenceValidatorTool()
        self.core_analyzer = CoreTechAnalyzerTool()
        self.ip_validator = IP_ResearchValidatorTool()
        self.competitive_agent = CompetitiveLandscapeAgent()
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
        
    def run(self, input_json: dict) -> dict:
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - Agentic ì›Œí¬í”Œë¡œìš°"""
        print("ğŸš€ TechSummaryAgent Agentic ì›Œí¬í”Œë¡œìš° ì‹œì‘")
        print(f"ğŸ“¥ ë¶„ì„ ëŒ€ìƒ: {input_json.get('company_name', '')} - {input_json.get('core_tech', '')}")
        
        # ì…ë ¥ ë°ì´í„°ë¥¼ ìƒíƒœë¡œ ë³€í™˜
        state = TechAnalysisState(
            company_name=input_json.get("company_name", ""),
            core_tech=input_json.get("core_tech", ""),
            patents=input_json.get("patents", ""),
            investments=input_json.get("investments", ""),
            pros=input_json.get("pros", ""),
            owner=input_json.get("owner", ""),
            tech_cache={},
            ip_cache={},
            competition_cache={},
            tech_summary="",
            strengths_and_weaknesses="",
            differentiation_points="",
            technical_risks="",
            patents_and_papers=[],
            tech_exists=False,
            analysis_complete=False
        )
        
        try:
            # Agentic ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            print("\nğŸ”„ 1ë‹¨ê³„: ê¸°ìˆ  ì‹¤ì¡´ì„± ê²€ì¦ (ì›¹ê²€ìƒ‰ í¬í•¨)")
            state = self.tech_validator.run(state)
            
            # ì¡°ê±´ë¶€ ë¶„ê¸°
            if state.get("tech_exists", False):
                print("\nğŸ”„ 2ë‹¨ê³„: í•µì‹¬ ê¸°ìˆ  ë¶„ì„ (ì›¹ê²€ìƒ‰ ë³´ê°•)")
                state = self.core_analyzer.run(state)
                
                print("\nğŸ”„ 3ë‹¨ê³„: IP ê²€ì¦ (ì›¹ê²€ìƒ‰ êµì°¨ê²€ì¦)")
                state = self.ip_validator.run(state)
                
                print("\nğŸ”„ 4ë‹¨ê³„: ê²½ìŸ í™˜ê²½ ë¶„ì„ (ì›¹ê²€ìƒ‰ ê¸°ë°˜)")
                state = self.competitive_agent.run(state)
                
                print("\nğŸ”„ 5ë‹¨ê³„: ìµœì¢… ìš”ì•½ ìƒì„±")
                state = self._generate_final_summary(state)
            else:
                print("\nğŸ”„ ê¸°ìˆ  ë¯¸í™•ì¸ - ê¸°ë³¸ ìš”ì•½ ìƒì„±")
                state = self._generate_basic_summary(state)
            
            # ê²°ê³¼ë¥¼ ì›ë³¸ ë°ì´í„°ì™€ ë³‘í•©
            result = input_json.copy()
            result.update({
                "tech_summary": state.get("tech_summary", ""),
                "strengths_and_weaknesses": state.get("strengths_and_weaknesses", ""),
                "differentiation_points": state.get("differentiation_points", ""),
                "technical_risks": state.get("technical_risks", ""),
                "patents_and_papers": state.get("patents_and_papers", [])
            })
            
            # JSON íŒŒì¼ ì—…ë°ì´íŠ¸
            self._update_checkpoint_file(result)
            
            print("ğŸ‰ TechSummaryAgent Agentic ì›Œí¬í”Œë¡œìš° ì™„ë£Œ")
            return result
            
        except Exception as e:
            print(f"âŒ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            result = input_json.copy()
            result.update({
                "tech_summary": f"ê¸°ìˆ  ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "strengths_and_weaknesses": "ë¶„ì„ ë¶ˆê°€",
                "differentiation_points": "ë¶„ì„ ë¶ˆê°€",
                "technical_risks": "ë¶„ì„ ë¶ˆê°€",
                "patents_and_papers": []
            })
            return result
    
    def _generate_final_summary(self, state: TechAnalysisState) -> TechAnalysisState:
        """ìµœì¢… ìš”ì•½ ìƒì„±"""
        print("ğŸ“ ìµœì¢… ìš”ì•½ ìƒì„±")
        
        try:
            tech_cache = getattr(state, 'tech_cache', {})
            ip_cache = getattr(state, 'ip_cache', {})
            competition_cache = getattr(state, 'competition_cache', {})
            
            summary_prompt = PromptTemplate(
                template="""ë‹¤ìŒ ì¢…í•© ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ íˆ¬ì ê´€ì ì˜ ê¸°ìˆ  ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”:

íšŒì‚¬: {company}
í•µì‹¬ ê¸°ìˆ : {tech}
ì†Œìœ ì: {owner}
ê¸°ìˆ  ë¶„ì„: {tech_analysis}
íŠ¹í—ˆ ë¶„ì„: {ip_analysis}
ê²½ìŸ ë¶„ì„: {competition_analysis}

ë‹¤ìŒ 5ê°œ í•­ëª©ì„ íˆ¬ìì ê´€ì ì—ì„œ ì‘ì„±í•˜ì„¸ìš”:

1. TECH_SUMMARY (ê¸°ìˆ  ìš”ì•½ - 400ì ì´ë‚´):
   - í•µì‹¬ ê¸°ìˆ ì˜ ë³¸ì§ˆê³¼ ì‘ë™ ì›ë¦¬
   - ê¸°ìˆ ì˜ í˜ì‹ ì„±ê³¼ ì‹œì¥ ì ìš© ê°€ëŠ¥ì„±
   - íˆ¬ì ê°€ì¹˜ ê´€ì ì—ì„œì˜ ê¸°ìˆ ë ¥ í‰ê°€

2. STRENGTHS_WEAKNESSES (ê°•ì /ì•½ì  - ê° 200ì ì´ë‚´):
   ê°•ì : ê¸°ìˆ ì  ìš°ìˆ˜ì„±, íŠ¹í—ˆ ê²½ìŸë ¥, ì‹œì¥ ì„ ë„ ê°€ëŠ¥ì„±
   ì•½ì : ê¸°ìˆ ì  í•œê³„, êµ¬í˜„ ë‚œì´ë„, ì‹œì¥ ì§„ì… ì¥ë²½

3. DIFFERENTIATION (ì°¨ë³„ì  - 300ì ì´ë‚´):
   - ê²½ìŸì‚¬ ëŒ€ë¹„ í•µì‹¬ ì°¨ë³„í™” ìš”ì†Œ
   - ê¸°ìˆ ì  í•´ì(Moat)ì™€ ëª¨ë°© ë‚œì´ë„
   - ì§€ì† ê°€ëŠ¥í•œ ê²½ìŸ ìš°ìœ„

4. TECHNICAL_RISKS (ê¸°ìˆ  ë¦¬ìŠ¤í¬ - 300ì ì´ë‚´):
   - ê¸°ìˆ  êµ¬í˜„ì˜ ë‚œì´ë„ì™€ ë¶ˆí™•ì‹¤ì„±
   - í™•ì¥ì„± ë° ìƒìš©í™” ë¦¬ìŠ¤í¬
   - ë°ì´í„° ì˜ì¡´ì„±, ê·œì œ, ì‹œì¥ ìˆ˜ìš©ì„± ë¦¬ìŠ¤í¬

5. PATENTS_PAPERS (íŠ¹í—ˆ/ë…¼ë¬¸ - êµ¬ì²´ì  ëª©ë¡):
   - ë³´ìœ  íŠ¹í—ˆ í˜„í™©ê³¼ ê¸°ìˆ  ë¶„ì•¼
   - ë°œí‘œ ë…¼ë¬¸ ë° ì—°êµ¬ ì„±ê³¼
   - ì§€ì ì¬ì‚°ê¶Œ ê²½ìŸë ¥ê³¼ ë°©ì–´ ëŠ¥ë ¥

ê° í•­ëª©ì€ ë°˜ë“œì‹œ í•´ë‹¹ ë¼ë²¨ë¡œ ì‹œì‘í•˜ì„¸ìš”.""",
                input_variables=["company", "tech", "owner", "tech_analysis", "ip_analysis", "competition_analysis"]
            )
            
            chain = summary_prompt | self.llm | StrOutputParser()
            summary = chain.invoke({
                "company": state.get("company_name", ""),
                "tech": state.get("core_tech", ""),
                "owner": state.get("owner", ""),
                "tech_analysis": str(tech_cache),
                "ip_analysis": str(ip_cache),
                "competition_analysis": str(competition_cache)
            })
            
            # ê²°ê³¼ íŒŒì‹± ë° ì €ì¥
            state.update({
                "tech_summary": self._extract_section(summary, "TECH_SUMMARY"),
                "strengths_and_weaknesses": self._extract_section(summary, "STRENGTHS_WEAKNESSES"),
                "differentiation_points": self._extract_section(summary, "DIFFERENTIATION"),
                "technical_risks": self._extract_section(summary, "TECHNICAL_RISKS"),
                "patents_and_papers": self._parse_patents_papers(self._extract_section(summary, "PATENTS_PAPERS")),
                "analysis_complete": True
            })
            
            print("âœ… ìµœì¢… ìš”ì•½ ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
            state.update({
                "tech_summary": f"{state.get('company_name', '')}ì˜ {state.get('core_tech', '')} ê¸°ìˆ  ë¶„ì„",
                "strengths_and_weaknesses": "ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼",
                "differentiation_points": "ê²½ìŸì‚¬ ëŒ€ë¹„ ì°¨ë³„í™” ìš”ì†Œ ì‹ë³„ë¨",
                "technical_risks": "ê¸°ìˆ  êµ¬í˜„ ë° ì‹œì¥ ì§„ì… ë¦¬ìŠ¤í¬ ì¡´ì¬",
                "patents_and_papers": [],
                "analysis_complete": False
            })
        
        return state
    
    def _generate_basic_summary(self, state: TechAnalysisState) -> TechAnalysisState:
        """ê¸°ë³¸ ìš”ì•½ ìƒì„±"""
        print("ğŸ“ ê¸°ë³¸ ìš”ì•½ ìƒì„±")
        
        state.update({
            "tech_summary": f"{state.get('company_name', '')}ì˜ {state.get('core_tech', '')} - ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ê¸°ë³¸ ë¶„ì„",
            "strengths_and_weaknesses": "ì œí•œëœ ì •ë³´ë¡œ ì¸í•œ ê¸°ë³¸ ë¶„ì„, ì¶”ê°€ ê²€ì¦ í•„ìš”",
            "differentiation_points": "ì°¨ë³„ì  ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ í•„ìš”",
            "technical_risks": "ê¸°ìˆ  ê²€ì¦ ë¶€ì¡±ìœ¼ë¡œ ì¸í•œ ë†’ì€ ë¶ˆí™•ì‹¤ì„±",
            "patents_and_papers": [],
            "analysis_complete": False
        })
        
        return state
    
    def _update_checkpoint_file(self, result: dict):
        """ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—…ë°ì´íŠ¸"""
        BASE_DIR = os.path.dirname(os.path.abspath(__file__))   # .../skala_rag_project/skala_rag_project/agents
        # checkpoint í´ë”ëŠ” agentsì™€ ê°™ì€ ë ˆë²¨
        CHECKPOINT_PATH = os.path.join(BASE_DIR, "..", "checkpoint", "01_company_desc_semantic.json")
        # ê²½ë¡œ ì •ê·œí™” (.. ì²˜ë¦¬)
        checkpoint_path = os.path.normpath(CHECKPOINT_PATH)
        
        try:
            # ê¸°ì¡´ íŒŒì¼ ì½ê¸°
            if os.path.exists(checkpoint_path):
                with open(checkpoint_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = []
            
            # í•´ë‹¹ íšŒì‚¬ ë°ì´í„° ì°¾ì•„ì„œ ì—…ë°ì´íŠ¸
            company_name = result.get("company_name", "")
            updated = False
            
            for i, item in enumerate(data):
                if item.get("company_name") == company_name:
                    # ê¸°ì¡´ ë°ì´í„°ì— ìƒˆë¡œìš´ í•„ë“œ ì¶”ê°€
                    data[i].update({
                        "tech_summary": result.get("tech_summary", ""),
                        "strengths_and_weaknesses": result.get("strengths_and_weaknesses", ""),
                        "differentiation_points": result.get("differentiation_points", ""),
                        "technical_risks": result.get("technical_risks", ""),
                        "patents_and_papers": result.get("patents_and_papers", [])
                    })
                    updated = True
                    break
            
            # ìƒˆë¡œìš´ íšŒì‚¬ë©´ ì¶”ê°€
            if not updated:
                data.append(result)
            
            # íŒŒì¼ ì €ì¥
            os.makedirs(os.path.dirname(checkpoint_path), exist_ok=True)
            with open(checkpoint_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {checkpoint_path}")
            
        except Exception as e:
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def _extract_section(self, text: str, section: str) -> str:
        """ì„¹ì…˜ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        lines = text.split('\n')
        content = []
        capturing = False
        
        for line in lines:
            if section in line:
                capturing = True
                if ':' in line:
                    content.append(line.split(':', 1)[1].strip())
                continue
            elif capturing and any(label in line for label in ["TECH_SUMMARY", "STRENGTHS_WEAKNESSES", "DIFFERENTIATION", "TECHNICAL_RISKS", "PATENTS_PAPERS"]):
                break
            elif capturing and line.strip():
                content.append(line.strip())
        
        return '\n'.join(content).strip() if content else "ì •ë³´ ì—†ìŒ"
    
    def _parse_patents_papers(self, text: str) -> List[Dict]:
        """íŠ¹í—ˆ/ë…¼ë¬¸ ì •ë³´ë¥¼ êµ¬ì¡°í™”ëœ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        if not text or text == "ì •ë³´ ì—†ìŒ":
            return []
        
        items = []
        lines = text.split('\n')
        for line in lines:
            if line.strip():
                items.append({
                    "type": "patent" if "íŠ¹í—ˆ" in line else "paper",
                    "description": line.strip()
                })
        
        return items[:10]  # ìµœëŒ€ 10ê°œ í•­ëª©